{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Loading the saved model points"
      ],
      "metadata": {
        "id": "3qstcIPTZKR4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Receiver Model"
      ],
      "metadata": {
        "id": "PvcY8RM4ZP5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from transformers import RobertaTokenizer, RobertaModel, get_linear_schedule_with_warmup\n",
        "from torch_geometric.nn import GATConv\n",
        "import networkx as nx\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "import json\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "y9lw7rfmTvAc"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTHYuVK7bmVM",
        "outputId": "27871210-8997-402b-f24b-0f703186f3f6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.4.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.19.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "eOoS84OgTyrJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LabelSmoothingLoss(nn.Module):\n",
        "    def __init__(self, smoothing=0.1):\n",
        "        super().__init__()\n",
        "        self.smoothing = smoothing\n",
        "        self.confidence = 1.0 - smoothing\n",
        "\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        logprobs = F.log_softmax(logits, dim=-1)\n",
        "        nll_loss = -logprobs.gather(dim=-1, index=targets.unsqueeze(1))\n",
        "        nll_loss = nll_loss.squeeze(1)\n",
        "        smooth_loss = -logprobs.mean(dim=-1)\n",
        "        loss = self.confidence * nll_loss + self.smoothing * smooth_loss\n",
        "        return loss.mean()\n"
      ],
      "metadata": {
        "id": "JlnlADI9T0ov"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphPooling(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "\n",
        "    def forward(self, node_feats):\n",
        "        weights = torch.softmax(self.attn(node_feats), dim=0)\n",
        "        return torch.sum(weights * node_feats, dim=0)"
      ],
      "metadata": {
        "id": "89AFGpocT21i"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, heads=4):\n",
        "        super().__init__()\n",
        "        self.gat1 = GATConv(input_dim, hidden_dim, heads=heads, dropout=0.2)\n",
        "        self.gat2 = GATConv(hidden_dim * heads, hidden_dim, heads=1, dropout=0.2)\n",
        "        self.res_fc = nn.Linear(input_dim, hidden_dim) if input_dim != hidden_dim else nn.Identity()\n",
        "\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x_res = self.res_fc(x)\n",
        "        x = F.elu(self.gat1(x, edge_index))\n",
        "        x = F.elu(self.gat2(x, edge_index) + x_res)\n",
        "        return x"
      ],
      "metadata": {
        "id": "ZxS3kPIQT43N"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PowerEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(input_dim, hidden_dim)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.leaky_relu(self.fc(x), negative_slope=0.2)"
      ],
      "metadata": {
        "id": "fumtZj4rT7Zb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DualAttention(nn.Module):\n",
        "    def __init__(self, hidden_dim, heads=4):\n",
        "        super().__init__()\n",
        "        self.heads = heads\n",
        "        self.query = nn.Linear(hidden_dim, hidden_dim * heads)\n",
        "        self.key = nn.Linear(hidden_dim, hidden_dim * heads)\n",
        "        self.value = nn.Linear(hidden_dim, hidden_dim * heads)\n",
        "        self.out_proj = nn.Linear(hidden_dim * heads, hidden_dim)\n",
        "\n",
        "\n",
        "    def forward(self, text_emb, graph_emb):\n",
        "        combined = torch.stack([text_emb, graph_emb], dim=1)\n",
        "        q = self.query(combined).view(combined.size(0), 2, self.heads, -1).transpose(1, 2)\n",
        "        k = self.key(combined).view(combined.size(0), 2, self.heads, -1).transpose(1, 2)\n",
        "        v = self.value(combined).view(combined.size(0), 2, self.heads, -1).transpose(1, 2)\n",
        "\n",
        "\n",
        "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / (q.size(-1) ** 0.5)\n",
        "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
        "        context = torch.matmul(attn_weights, v)\n",
        "        context = context.transpose(1, 2).reshape(q.size(0), 2, -1).mean(dim=1)\n",
        "        return self.out_proj(context)"
      ],
      "metadata": {
        "id": "9pJoOcVbT_pH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeceptionDetector(nn.Module):\n",
        "    def __init__(self, roberta_model_name='roberta-base', power_dim=10, graph_feat_dim=128, hidden_dim=256):\n",
        "        super().__init__()\n",
        "        self.roberta = RobertaModel.from_pretrained(roberta_model_name)\n",
        "        roberta_dim = self.roberta.config.hidden_size\n",
        "        self.roberta_proj = nn.Linear(roberta_dim, hidden_dim) if roberta_dim != hidden_dim else nn.Identity()\n",
        "\n",
        "\n",
        "        self.power_encoder = PowerEncoder(power_dim, hidden_dim)\n",
        "        self.graph_encoder = GraphEncoder(graph_feat_dim, hidden_dim)\n",
        "        self.graph_pool = GraphPooling(hidden_dim)\n",
        "        self.dual_attn = DualAttention(hidden_dim)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.classifier = nn.Linear(hidden_dim, 2)\n",
        "\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, power_feats, graph_feats, edge_indices):\n",
        "        roberta_output = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        roberta_cls = self.roberta_proj(roberta_output.last_hidden_state[:, 0, :])\n",
        "        power_encoded = self.power_encoder(power_feats)\n",
        "        text_feat = self.dropout(roberta_cls + power_encoded)\n",
        "\n",
        "\n",
        "        all_graph_feats = torch.cat(graph_feats, dim=0)\n",
        "        edge_indices = [edge_index + i * graph_feats[i].size(0) for i, edge_index in enumerate(edge_indices)]\n",
        "        batched_edge_index = torch.cat(edge_indices, dim=1)\n",
        "        graph_feat = self.graph_encoder(all_graph_feats, batched_edge_index)\n",
        "\n",
        "\n",
        "        split_sizes = [gf.size(0) for gf in graph_feats]\n",
        "        graph_feat = torch.split(graph_feat, split_sizes)\n",
        "        graph_feat = torch.stack([self.graph_pool(g) for g in graph_feat])\n",
        "        graph_feat = self.dropout(graph_feat)\n",
        "\n",
        "\n",
        "        final_feat = self.dual_attn(text_feat, graph_feat)\n",
        "        return self.classifier(final_feat)"
      ],
      "metadata": {
        "id": "vjIsGL3BUIKZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DiplomacyDataset(Dataset):\n",
        "    def __init__(self, jsonl_path, tokenizer, max_len=128):\n",
        "        self.samples = []\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "\n",
        "        for line in open(jsonl_path):\n",
        "            game_data = json.loads(line)\n",
        "            self.process_game(game_data)\n",
        "\n",
        "\n",
        "    def process_game(self, game_data):\n",
        "        game_state = {\n",
        "            'power_scores': {},\n",
        "            'message_counts': {},\n",
        "            'recent_deceptions': {},\n",
        "            'relationships': set()\n",
        "        }\n",
        "\n",
        "\n",
        "        players = list(set(game_data['speakers'] + [r for r in game_data['receivers'] if r != \"NOANNOTATION\"]))\n",
        "        for player in players:\n",
        "            game_state['power_scores'][player] = 0\n",
        "            game_state['message_counts'][player] = 0\n",
        "            game_state['recent_deceptions'][player] = []\n",
        "\n",
        "\n",
        "        G = nx.Graph()\n",
        "        for sender, receiver in zip(game_data['speakers'], game_data['receivers']):\n",
        "            if receiver != \"NOANNOTATION\":\n",
        "                G.add_edge(sender, receiver)\n",
        "        centrality = nx.betweenness_centrality(G)\n",
        "\n",
        "\n",
        "        for i, (msg, sender, receiver, r_label) in enumerate(zip(\n",
        "            game_data['messages'], game_data['speakers'], game_data['receivers'], game_data['receiver_labels']\n",
        "        )):\n",
        "            if receiver == \"NOANNOTATION\":\n",
        "                continue\n",
        "\n",
        "\n",
        "            game_state['message_counts'][sender] += 1\n",
        "            game_state['power_scores'][sender] = float(game_data['game_score'][i])\n",
        "\n",
        "\n",
        "            if not r_label:\n",
        "                game_state['recent_deceptions'][sender].append(1)\n",
        "                if len(game_state['recent_deceptions'][sender]) > 5:\n",
        "                    game_state['recent_deceptions'][sender].pop(0)\n",
        "\n",
        "\n",
        "            game_state['relationships'].add((sender, receiver))\n",
        "            game_state['relationships'].add((receiver, sender))\n",
        "\n",
        "\n",
        "            power_feats = self.get_power_features(game_data, i, game_state)\n",
        "            graph_feats = self.get_graph_features(players, game_state, centrality)\n",
        "            edge_index = self.get_edge_index(players, game_state)\n",
        "\n",
        "\n",
        "            self.samples.append({\n",
        "                'text': msg,\n",
        "                'label': 1 if r_label else 0,\n",
        "                'power_feats': power_feats,\n",
        "                'graph_feats': graph_feats,\n",
        "                'edge_index': edge_index\n",
        "            })\n",
        "\n",
        "\n",
        "    def get_power_features(self, game_data, msg_idx, game_state):\n",
        "        sender = game_data['speakers'][msg_idx]\n",
        "        receiver = game_data['receivers'][msg_idx]\n",
        "\n",
        "\n",
        "        decayed_deception = sum([d * (0.9 ** t) for t, d in enumerate(reversed(game_state['recent_deceptions'][sender]))])\n",
        "\n",
        "\n",
        "        features = [\n",
        "            float(game_data['game_score_delta'][msg_idx]),\n",
        "            game_state['power_scores'][sender] - game_state['power_scores'].get(receiver, 0),\n",
        "            int(game_data['years'][msg_idx]) - 1901,\n",
        "            1 if game_data['seasons'][msg_idx] == 'Spring' else 0,\n",
        "            1 if game_data['seasons'][msg_idx] == 'Fall' else 0,\n",
        "            game_state['message_counts'][sender] / (sum(game_state['message_counts'].values()) + 1e-6),\n",
        "            decayed_deception,\n",
        "            msg_idx / len(game_data['messages']),\n",
        "            game_data['relative_message_index'][msg_idx] / 100,\n",
        "            len([d for d in game_state['recent_deceptions'][sender] if d == 1]) / 5.0\n",
        "        ]\n",
        "        return features[:10]\n",
        "\n",
        "\n",
        "    def get_graph_features(self, players, game_state, centrality):\n",
        "        player_features = []\n",
        "        for player in players:\n",
        "            features = [\n",
        "                game_state['power_scores'][player],\n",
        "                game_state['message_counts'][player],\n",
        "                sum(game_state['recent_deceptions'][player]) / 5.0,\n",
        "                centrality.get(player, 0),\n",
        "                sum(1 for p in players if (player, p) in game_state['relationships'])\n",
        "            ]\n",
        "            features = torch.tensor(features, dtype=torch.float)\n",
        "            features = (features - features.mean()) / (features.std() + 1e-5)\n",
        "            padded = F.pad(features, (0, 128 - features.size(0)))\n",
        "            player_features.append(padded.tolist())\n",
        "        return player_features\n",
        "\n",
        "\n",
        "    def get_edge_index(self, players, game_state):\n",
        "        player_to_idx = {p: i for i, p in enumerate(players)}\n",
        "        edges = [[player_to_idx[p1], player_to_idx[p2]] for (p1, p2) in game_state['relationships']]\n",
        "        return torch.tensor(edges or [[0, 1], [1, 0]], dtype=torch.long).t().contiguous()\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "        encoded = self.tokenizer(sample['text'], padding='max_length', truncation=True,\n",
        "                               max_length=self.max_len, return_tensors='pt')\n",
        "        return {\n",
        "            'input_ids': encoded['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoded['attention_mask'].squeeze(0),\n",
        "            'power_feats': torch.tensor(sample['power_feats'], dtype=torch.float),\n",
        "            'graph_feats': torch.tensor(sample['graph_feats'], dtype=torch.float),\n",
        "            'edge_index': sample['edge_index'],\n",
        "            'label': torch.tensor(sample['label'], dtype=torch.long)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "YVKRDu-0URr8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    return {\n",
        "        'input_ids': torch.stack([item['input_ids'] for item in batch]),\n",
        "        'attention_mask': torch.stack([item['attention_mask'] for item in batch]),\n",
        "        'power_feats': torch.stack([item['power_feats'] for item in batch]),\n",
        "        'graph_feats': [item['graph_feats'] for item in batch],\n",
        "        'edge_index': [item['edge_index'] for item in batch],\n",
        "        'label': torch.stack([item['label'] for item in batch])\n",
        "    }"
      ],
      "metadata": {
        "id": "KDYymwUeUZLJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, dataloader, optimizer, scheduler, criterion, device, grad_accum_steps=4):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    all_preds, all_labels = [], []\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "\n",
        "    for i, batch in enumerate(dataloader):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        power_feats = batch['power_feats'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "        graph_feats = [gf.to(device) for gf in batch['graph_feats']]\n",
        "        edge_indices = [ei.to(device) for ei in batch['edge_index']]\n",
        "\n",
        "\n",
        "        logits = model(input_ids, attention_mask, power_feats, graph_feats, edge_indices)\n",
        "        loss = criterion(logits, labels) / grad_accum_steps\n",
        "        loss.backward()\n",
        "\n",
        "\n",
        "        if (i + 1) % grad_accum_steps == 0:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "\n",
        "        total_loss += loss.item() * grad_accum_steps\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        all_preds.extend(preds.cpu().tolist())\n",
        "        all_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    return avg_loss, acc, f1"
      ],
      "metadata": {
        "id": "CEWzowYAUgeM"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    all_texts = []\n",
        "\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    total_loss = 0\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = torch.stack([item['input_ids'] for item in batch]).to(device)\n",
        "            attention_mask = torch.stack([item['attention_mask'] for item in batch]).to(device)\n",
        "            power_feats = torch.stack([item['power_feats'] for item in batch]).to(device)\n",
        "            labels = torch.stack([item['label'] for item in batch]).to(device)\n",
        "            graph_feats = [item['graph_feats'].to(device) for item in batch]\n",
        "            edge_indices = [item['edge_index'].to(device) for item in batch]\n",
        "\n",
        "\n",
        "            logits = model(input_ids, attention_mask, power_feats, graph_feats, edge_indices)\n",
        "            loss = criterion(logits, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            all_preds.extend(preds.cpu().tolist())\n",
        "            all_labels.extend(labels.cpu().tolist())\n",
        "            texts = tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n",
        "            all_texts.extend(texts)\n",
        "\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    print(\"\\nSample predictions:\")\n",
        "    for i in range(min(5, len(all_texts))):\n",
        "        print(f\"Text: {all_texts[i]}\")\n",
        "        print(f\"True Label: {all_labels[i]} | Predicted: {all_preds[i]}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "\n",
        "    return avg_loss, acc, f1, all_preds, all_labels, all_texts\n"
      ],
      "metadata": {
        "id": "_mC8-iOiUmzd"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1kfSmoFTRZF",
        "outputId": "03880b61-18ea-4287-9b47-bc94dc099b04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# ---------------------- Main Execution ----------------------\n",
        "if __name__ == '__main__':\n",
        "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "    test_dataset = DiplomacyDataset('/content/test.jsonl', tokenizer)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=8, collate_fn=lambda x: x)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = DeceptionDetector().to(device)\n",
        "    model.load_state_dict(torch.load('/content/best_model_receiver.pt', map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    test_loss, test_acc, test_f1, preds, labels, texts = evaluate(model, test_loader, device)\n",
        "\n",
        "\n",
        "    print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
        "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "    print(f\"Test F1 Score: {test_f1:.4f}\")\n",
        "    with open('test_predictions_receiver.csv', 'w') as f:\n",
        "        f.write(\"text,true_label,predicted_label\\n\")\n",
        "        for text, true, pred in zip(texts, labels, preds):\n",
        "            f.write(f'\"{text}\",{true},{pred}\\n')\n",
        "\n",
        "\n",
        "    print(\"\\nPredictions saved to test_predictions_receiver.csv\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Sender model"
      ],
      "metadata": {
        "id": "MC0uZe4DZGYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from transformers import RobertaTokenizer, RobertaModel, get_linear_schedule_with_warmup\n",
        "from torch_geometric.nn import GATConv\n",
        "import networkx as nx\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "import json\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "9-jeV7ECZFqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "6tSUENtqars4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LabelSmoothingLoss(nn.Module):\n",
        "    def __init__(self, smoothing=0.1):\n",
        "        super().__init__()\n",
        "        self.smoothing = smoothing\n",
        "        self.confidence = 1.0 - smoothing\n",
        "\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        logprobs = F.log_softmax(logits, dim=-1)\n",
        "        nll_loss = -logprobs.gather(dim=-1, index=targets.unsqueeze(1))\n",
        "        nll_loss = nll_loss.squeeze(1)\n",
        "        smooth_loss = -logprobs.mean(dim=-1)\n",
        "        loss = self.confidence * nll_loss + self.smoothing * smooth_loss\n",
        "        return loss.mean()"
      ],
      "metadata": {
        "id": "3EiNOODmauyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphPooling(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "\n",
        "    def forward(self, node_feats):\n",
        "        weights = torch.softmax(self.attn(node_feats), dim=0)\n",
        "        return torch.sum(weights * node_feats, dim=0)"
      ],
      "metadata": {
        "id": "QsQz9s9oaxsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, heads=4):\n",
        "        super().__init__()\n",
        "        self.gat1 = GATConv(input_dim, hidden_dim, heads=heads, dropout=0.2)\n",
        "        self.gat2 = GATConv(hidden_dim * heads, hidden_dim, heads=1, dropout=0.2)\n",
        "        self.res_fc = nn.Linear(input_dim, hidden_dim) if input_dim != hidden_dim else nn.Identity()\n",
        "\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x_res = self.res_fc(x)\n",
        "        x = F.elu(self.gat1(x, edge_index))\n",
        "        x = F.elu(self.gat2(x, edge_index) + x_res)\n",
        "        return x"
      ],
      "metadata": {
        "id": "08sfkACra1uw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PowerEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(input_dim, hidden_dim)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.leaky_relu(self.fc(x), negative_slope=0.2)"
      ],
      "metadata": {
        "id": "4awfN5MEa4ec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DualAttention(nn.Module):\n",
        "    def __init__(self, hidden_dim, heads=4):\n",
        "        super().__init__()\n",
        "        self.heads = heads\n",
        "        self.query = nn.Linear(hidden_dim, hidden_dim * heads)\n",
        "        self.key = nn.Linear(hidden_dim, hidden_dim * heads)\n",
        "        self.value = nn.Linear(hidden_dim, hidden_dim * heads)\n",
        "        self.out_proj = nn.Linear(hidden_dim * heads, hidden_dim)\n",
        "\n",
        "\n",
        "    def forward(self, text_emb, graph_emb):\n",
        "        combined = torch.stack([text_emb, graph_emb], dim=1)\n",
        "        q = self.query(combined).view(combined.size(0), 2, self.heads, -1).transpose(1, 2)\n",
        "        k = self.key(combined).view(combined.size(0), 2, self.heads, -1).transpose(1, 2)\n",
        "        v = self.value(combined).view(combined.size(0), 2, self.heads, -1).transpose(1, 2)\n",
        "\n",
        "\n",
        "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / (q.size(-1) ** 0.5)\n",
        "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
        "        context = torch.matmul(attn_weights, v)\n",
        "        context = context.transpose(1, 2).reshape(q.size(0), 2, -1).mean(dim=1)\n",
        "        return self.out_proj(context)"
      ],
      "metadata": {
        "id": "oDcLOofBa8dH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeceptionDetector(nn.Module):\n",
        "    def __init__(self, roberta_model_name='roberta-base', power_dim=10, graph_feat_dim=128, hidden_dim=256):\n",
        "        super().__init__()\n",
        "        self.roberta = RobertaModel.from_pretrained(roberta_model_name)\n",
        "        roberta_dim = self.roberta.config.hidden_size\n",
        "        self.roberta_proj = nn.Linear(roberta_dim, hidden_dim) if roberta_dim != hidden_dim else nn.Identity()\n",
        "\n",
        "\n",
        "        self.power_encoder = PowerEncoder(power_dim, hidden_dim)\n",
        "        self.graph_encoder = GraphEncoder(graph_feat_dim, hidden_dim)\n",
        "        self.graph_pool = GraphPooling(hidden_dim)\n",
        "        self.dual_attn = DualAttention(hidden_dim)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.classifier = nn.Linear(hidden_dim, 2)\n",
        "\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, power_feats, graph_feats, edge_indices):\n",
        "        roberta_output = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        roberta_cls = self.roberta_proj(roberta_output.last_hidden_state[:, 0, :])\n",
        "        power_encoded = self.power_encoder(power_feats)\n",
        "        text_feat = self.dropout(roberta_cls + power_encoded)\n",
        "\n",
        "\n",
        "        all_graph_feats = torch.cat(graph_feats, dim=0)\n",
        "        edge_indices = [edge_index + i * graph_feats[i].size(0) for i, edge_index in enumerate(edge_indices)]\n",
        "        batched_edge_index = torch.cat(edge_indices, dim=1)\n",
        "        graph_feat = self.graph_encoder(all_graph_feats, batched_edge_index)\n",
        "\n",
        "\n",
        "        split_sizes = [gf.size(0) for gf in graph_feats]\n",
        "        graph_feat = torch.split(graph_feat, split_sizes)\n",
        "        graph_feat = torch.stack([self.graph_pool(g) for g in graph_feat])\n",
        "        graph_feat = self.dropout(graph_feat)\n",
        "\n",
        "\n",
        "        final_feat = self.dual_attn(text_feat, graph_feat)\n",
        "        return self.classifier(final_feat)"
      ],
      "metadata": {
        "id": "0-Zk-NM1bAg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DiplomacyDataset(Dataset):\n",
        "    def __init__(self, jsonl_path, tokenizer, max_len=128):\n",
        "        self.samples = []\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "\n",
        "        for line in open(jsonl_path):\n",
        "            game_data = json.loads(line)\n",
        "            self.process_game(game_data)\n",
        "\n",
        "\n",
        "    def process_game(self, game_data):\n",
        "        game_state = {\n",
        "            'power_scores': {},\n",
        "            'message_counts': {},\n",
        "            'recent_deceptions': {},\n",
        "            'relationships': set()\n",
        "        }\n",
        "\n",
        "\n",
        "        players = list(set(game_data['speakers'] + [r for r in game_data['receivers'] if r != \"NOANNOTATION\"]))\n",
        "        for player in players:\n",
        "            game_state['power_scores'][player] = 0\n",
        "            game_state['message_counts'][player] = 0\n",
        "            game_state['recent_deceptions'][player] = []\n",
        "\n",
        "\n",
        "        G = nx.Graph()\n",
        "        for sender, receiver in zip(game_data['speakers'], game_data['receivers']):\n",
        "            if receiver != \"NOANNOTATION\":\n",
        "                G.add_edge(sender, receiver)\n",
        "        centrality = nx.betweenness_centrality(G)\n",
        "\n",
        "\n",
        "        for i, (msg, sender, receiver, r_label) in enumerate(zip(\n",
        "            game_data['messages'], game_data['speakers'], game_data['receivers'], game_data['sender_labels']\n",
        "        )):\n",
        "            if receiver == \"NOANNOTATION\":\n",
        "                continue\n",
        "\n",
        "\n",
        "            game_state['message_counts'][sender] += 1\n",
        "            game_state['power_scores'][sender] = float(game_data['game_score'][i])\n",
        "\n",
        "\n",
        "            if not r_label:\n",
        "                game_state['recent_deceptions'][sender].append(1)\n",
        "                if len(game_state['recent_deceptions'][sender]) > 5:\n",
        "                    game_state['recent_deceptions'][sender].pop(0)\n",
        "\n",
        "\n",
        "            game_state['relationships'].add((sender, receiver))\n",
        "            game_state['relationships'].add((receiver, sender))\n",
        "\n",
        "\n",
        "            power_feats = self.get_power_features(game_data, i, game_state)\n",
        "            graph_feats = self.get_graph_features(players, game_state, centrality)\n",
        "            edge_index = self.get_edge_index(players, game_state)\n",
        "\n",
        "\n",
        "            self.samples.append({\n",
        "                'text': msg,\n",
        "                'label': 1 if r_label else 0,\n",
        "                'power_feats': power_feats,\n",
        "                'graph_feats': graph_feats,\n",
        "                'edge_index': edge_index\n",
        "            })\n",
        "\n",
        "\n",
        "    def get_power_features(self, game_data, msg_idx, game_state):\n",
        "        sender = game_data['speakers'][msg_idx]\n",
        "        receiver = game_data['receivers'][msg_idx]\n",
        "\n",
        "\n",
        "        decayed_deception = sum([d * (0.9 ** t) for t, d in enumerate(reversed(game_state['recent_deceptions'][sender]))])\n",
        "\n",
        "\n",
        "        features = [\n",
        "            float(game_data['game_score_delta'][msg_idx]),\n",
        "            game_state['power_scores'][sender] - game_state['power_scores'].get(receiver, 0),\n",
        "            int(game_data['years'][msg_idx]) - 1901,\n",
        "            1 if game_data['seasons'][msg_idx] == 'Spring' else 0,\n",
        "            1 if game_data['seasons'][msg_idx] == 'Fall' else 0,\n",
        "            game_state['message_counts'][sender] / (sum(game_state['message_counts'].values()) + 1e-6),\n",
        "            decayed_deception,\n",
        "            msg_idx / len(game_data['messages']),\n",
        "            game_data['relative_message_index'][msg_idx] / 100,\n",
        "            len([d for d in game_state['recent_deceptions'][sender] if d == 1]) / 5.0\n",
        "        ]\n",
        "        return features[:10]\n",
        "\n",
        "\n",
        "    def get_graph_features(self, players, game_state, centrality):\n",
        "        player_features = []\n",
        "        for player in players:\n",
        "            features = [\n",
        "                game_state['power_scores'][player],\n",
        "                game_state['message_counts'][player],\n",
        "                sum(game_state['recent_deceptions'][player]) / 5.0,\n",
        "                centrality.get(player, 0),\n",
        "                sum(1 for p in players if (player, p) in game_state['relationships'])\n",
        "            ]\n",
        "            features = torch.tensor(features, dtype=torch.float)\n",
        "            features = (features - features.mean()) / (features.std() + 1e-5)\n",
        "            padded = F.pad(features, (0, 128 - features.size(0)))\n",
        "            player_features.append(padded.tolist())\n",
        "        return player_features\n",
        "\n",
        "\n",
        "    def get_edge_index(self, players, game_state):\n",
        "        player_to_idx = {p: i for i, p in enumerate(players)}\n",
        "        edges = [[player_to_idx[p1], player_to_idx[p2]] for (p1, p2) in game_state['relationships']]\n",
        "        return torch.tensor(edges or [[0, 1], [1, 0]], dtype=torch.long).t().contiguous()\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "        encoded = self.tokenizer(sample['text'], padding='max_length', truncation=True,\n",
        "                               max_length=self.max_len, return_tensors='pt')\n",
        "        return {\n",
        "            'input_ids': encoded['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoded['attention_mask'].squeeze(0),\n",
        "            'power_feats': torch.tensor(sample['power_feats'], dtype=torch.float),\n",
        "            'graph_feats': torch.tensor(sample['graph_feats'], dtype=torch.float),\n",
        "            'edge_index': sample['edge_index'],\n",
        "            'label': torch.tensor(sample['label'], dtype=torch.long)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "0vmxR_NobEyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    return {\n",
        "        'input_ids': torch.stack([item['input_ids'] for item in batch]),\n",
        "        'attention_mask': torch.stack([item['attention_mask'] for item in batch]),\n",
        "        'power_feats': torch.stack([item['power_feats'] for item in batch]),\n",
        "        'graph_feats': [item['graph_feats'] for item in batch],\n",
        "        'edge_index': [item['edge_index'] for item in batch],\n",
        "        'label': torch.stack([item['label'] for item in batch])\n",
        "    }"
      ],
      "metadata": {
        "id": "PdBvJfxZbJ1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, dataloader, optimizer, scheduler, criterion, device, grad_accum_steps=4):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    all_preds, all_labels = [], []\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "\n",
        "    for i, batch in enumerate(dataloader):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        power_feats = batch['power_feats'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "        graph_feats = [gf.to(device) for gf in batch['graph_feats']]\n",
        "        edge_indices = [ei.to(device) for ei in batch['edge_index']]\n",
        "\n",
        "\n",
        "        logits = model(input_ids, attention_mask, power_feats, graph_feats, edge_indices)\n",
        "        loss = criterion(logits, labels) / grad_accum_steps\n",
        "        loss.backward()\n",
        "\n",
        "\n",
        "        if (i + 1) % grad_accum_steps == 0:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "\n",
        "        total_loss += loss.item() * grad_accum_steps\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        all_preds.extend(preds.cpu().tolist())\n",
        "        all_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    return avg_loss, acc, f1"
      ],
      "metadata": {
        "id": "6BmoTmonbOyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    all_texts = []\n",
        "\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    total_loss = 0\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = torch.stack([item['input_ids'] for item in batch]).to(device)\n",
        "            attention_mask = torch.stack([item['attention_mask'] for item in batch]).to(device)\n",
        "            power_feats = torch.stack([item['power_feats'] for item in batch]).to(device)\n",
        "            labels = torch.stack([item['label'] for item in batch]).to(device)\n",
        "            graph_feats = [item['graph_feats'].to(device) for item in batch]\n",
        "            edge_indices = [item['edge_index'].to(device) for item in batch]\n",
        "\n",
        "\n",
        "            logits = model(input_ids, attention_mask, power_feats, graph_feats, edge_indices)\n",
        "            loss = criterion(logits, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            all_preds.extend(preds.cpu().tolist())\n",
        "            all_labels.extend(labels.cpu().tolist())\n",
        "            texts = tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n",
        "            all_texts.extend(texts)\n",
        "\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    print(\"\\nSample predictions:\")\n",
        "    for i in range(min(5, len(all_texts))):\n",
        "        print(f\"Text: {all_texts[i]}\")\n",
        "        print(f\"True Label: {all_labels[i]} | Predicted: {all_preds[i]}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "\n",
        "    return avg_loss, acc, f1, all_preds, all_labels, all_texts\n"
      ],
      "metadata": {
        "id": "9N8isgo4bTn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------- Main Execution ----------------------\n",
        "if __name__ == '__main__':\n",
        "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "    test_dataset = DiplomacyDataset('/content/test.jsonl', tokenizer)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=8, collate_fn=lambda x: x)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = DeceptionDetector().to(device)\n",
        "    model.load_state_dict(torch.load('/content/best_model_sender.pt', map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    test_loss, test_acc, test_f1, preds, labels, texts = evaluate(model, test_loader, device)\n",
        "\n",
        "\n",
        "    print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
        "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "    print(f\"Test F1 Score: {test_f1:.4f}\")\n",
        "    with open('test_predictions_sender.csv', 'w') as f:\n",
        "        f.write(\"text,true_label,predicted_label\\n\")\n",
        "        for text, true, pred in zip(texts, labels, preds):\n",
        "            f.write(f'\"{text}\",{true},{pred}\\n')\n",
        "\n",
        "\n",
        "    print(\"\\nPredictions saved to test_predictions_sender.csv\")\n",
        "\n"
      ],
      "metadata": {
        "id": "wuAoiSwxbX7y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}